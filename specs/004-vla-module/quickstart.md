# Quickstart: Vision-Language-Action (VLA) Module

## Overview
The Vision-Language-Action (VLA) module explains how language models, perception, and robotics actions converge to create autonomous humanoid behavior. This module is designed for AI/robotics students familiar with ROS 2, simulation, and navigation concepts.

## Prerequisites
- Understanding of ROS 2 concepts
- Basic knowledge of robotics navigation and perception
- Familiarity with simulation environments

## Module Structure
The VLA module consists of three interconnected chapters:

### 1. Voice-to-Action
- Learn how to convert spoken commands into robot actions
- Understand speech-to-text processing with OpenAI Whisper
- Explore techniques for translating voice commands into robot intents

### 2. Cognitive Planning with LLMs
- Discover how Large Language Models can be used for task decomposition
- Learn to convert natural language goals into executable ROS 2 action sequences
- Understand the cognitive layer of autonomous humanoid behavior

### 3. Capstone: The Autonomous Humanoid
- Implement an end-to-end VLA pipeline
- Integrate voice commands through planning → navigation → perception → manipulation
- Build a complete autonomous humanoid system

## Getting Started
1. Begin with the Voice-to-Action chapter to understand the foundational concepts
2. Proceed to Cognitive Planning with LLMs to learn about higher-level decision making
3. Complete the capstone chapter to integrate all components into a unified system

## Navigation
- Use the sidebar to navigate between chapters
- Each chapter builds on the previous one, so following the sequence is recommended
- Look for cross-references between chapters for integrated concepts

## Learning Outcomes
After completing this module, you will understand:
- How to process voice commands for robot control
- How to use LLMs for task planning in robotics
- How to integrate multiple systems into a complete autonomous humanoid
- The complete flow from voice command to robot manipulation